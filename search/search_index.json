{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to OAAX","text":"<p>OAAX serves as a bridge between popular AI frameworks and diverse hardware accelerators. Models developed in frameworks such as TensorFlow, PyTorch, Hugging Face, and others are first exported to the ONNX format, a widely adopted standard for interoperability. OAAX then connects ONNX models to a variety of hardware backends\u2014including CPUs, Intel, NVIDIA, DEEPX, EdgeCortix, Hailo, and more\u2014enabling seamless deployment across heterogeneous compute platforms without requiring framework- or vendor-specific integration.</p> <p>This is achieved by providing a unified conversion and runtime interfaces, enabling developers to convert ONNX models into hardware-specific formats and run them seamlessly across different platforms using a standardized API.  </p>"},{"location":"#terminology","title":"Terminology","text":"<p>Before delving into the OAAX standard, it's important to understand some key terms that are frequently used:</p> <ul> <li>OAAX: Open AI Accelerator eXchange, a standard (specification, recipe) for deploying AI models across different hardware accelerators.</li> <li>XPU, AI accelerator or AI hardware: Any processing unit that can execute AI models, such as GPUs, TPUs, or specialized AI accelerators.</li> <li>Compile or Convert: The process of converting an ONNX model into a format optimized and supported for a specific XPU.</li> <li>Runtime: The shared library (.so or .dll) that implements the OAAX runtime interface to interact with an XPU.</li> <li>Conversion Toolchain: The software that compiles ONNX models.</li> <li>Input/Output Tensors: Data structures that hold the input and output data for the model.</li> <li>Host or Runtime host: The software that interacts with runtime to offload model computation to the AI hardware.</li> <li>OAAX-compliant XPU: Refers to any AI accelerator that has an implementation of the OAAX standard, including both the conversion toolchain and the runtime library.</li> </ul>"},{"location":"#usage-workflow","title":"Usage workflow","text":"<p>To run an AI model on an OAAX-compliant XPU, a typical workflow looks like this:</p> <ol> <li>Convert the ONNX model into an XPU-specific OAAX bundle/binary using the provided toolchain.</li> <li>In the host application, load the OAAX runtime appropriate to the XPU.</li> <li>Initialize the runtime by calling <code>runtime_initialization()</code>.</li> <li>Load the model with <code>runtime_model_loading(\"./model.oaax\")</code>.</li> <li>Exchange data asynchronously:    a. Send inputs with <code>send_input(input_tensors)</code>.    b. Retrieve outputs with <code>receive_output(output_tensors_holder)</code>.</li> <li>When finished, clean up resources by calling <code>runtime_destruction()</code>.</li> </ol>"},{"location":"#outline","title":"Outline","text":"<ul> <li>To get started with using OAAX, please check out the hello world example.</li> <li>To learn about the OAAX specification, please check out the OAAX Specification Document.</li> <li>To check if an AI Accelerator is compliant with OAAX, please refer to this list.</li> <li>To contribute a new OAAX implementation, improve an existing one, develop an example or propose new change to the standard, please refer to the Contributing Guide.</li> <li>Have more questions? Please check out the FAQ.</li> </ul>"},{"location":"CPU/","title":"CPU","text":""},{"location":"CPU/#overview","title":"Overview","text":"<p>The CPU reference implementation is the baseline OAAX runtime and toolchain. It demonstrates the OAAX standard in practice and provides a portable way to run ONNX models on x86_64 and aarch64 CPUs across Linux and Windows.</p> <p>This page explains how to convert a model with the CPU toolchain and run it with the CPU runtime.</p>"},{"location":"CPU/#requirements","title":"Requirements","text":""},{"location":"CPU/#hardware","title":"Hardware","text":"<ul> <li>x86_64 or aarch64 CPU</li> <li>AVX2 instruction set support (for x86_64)</li> </ul>"},{"location":"CPU/#software","title":"Software","text":"<ul> <li>Debian 11 or higher </li> <li>Ubuntu 20.04 or higher</li> <li>Windows 10, 11 (only when running on x86_64)</li> </ul>"},{"location":"CPU/#usage","title":"Usage","text":""},{"location":"CPU/#runtime-library","title":"Runtime Library","text":"<p>The runtime library implements the OAAX's runtime interface for initializing, loading, running inference and destroying the runtime. The initialization in particular can be done without providing parameters by calling <code>int runtime_initialization();</code> directly or by providing them to <code>int runtime_initialization_with_args(int length, char **keys, void **values);</code>.</p> <p>The following is the list of supported parameters:</p> <ul> <li><code>log_level</code> (char *, default is \"2\"): The minimal log level for the runtime. This can be set to <code>0</code> for trace, <code>1</code> for debug, <code>2</code> for info, <code>3</code> for warnings, <code>4</code> for errors, <code>5</code> for critical and <code>6</code> to disable logging.</li> <li><code>log_file</code> (char *, default is \"runtime.log\"): The file to which the runtime logs will be written. If not specified, logs will be written to stdout.</li> <li><code>num_threads</code> (char *, default is \"4\"): The maximum number of threads that can be used by the runtime. The higher the number, the more CPU resources will be used, but the better the throughput.</li> </ul> <p>You can check out the examples repository for more details on how to use the runtime library: OAAX Examples.</p>"},{"location":"CPU/#conversion-toolchain","title":"Conversion Toolchain","text":"<p>The conversion toolchain is used to validate, optimize and simplify the ONNX models. At the end of the process it produces a simplified ONNX model.</p> <p>It can be used as follows:</p> <pre><code>docker run -v ./model:/model  oaax-cpu-toolchain:latest /model/model.onnx /model/output\n</code></pre> <p>The above command assumes that the model is located at <code>./model/model.onnx</code>. After a succesful conversion, the generated model will be saved in the <code>./model/output</code> directory.</p>"},{"location":"CPU/#download-links-and-compatibility-matrix","title":"Download links and compatibility matrix","text":"OAAX version OS name OS version CPU architecture Runtime library Conversion toolchain 1.1.2 Ubuntu 20.04 or higher x86_64 Download Download 1.1.2 Ubuntu 20.04 or higher aarch64 Download \u2b06\ufe0f 1.1.2 Windows 10, 11 x86_64 Download \u2b06\ufe0f"},{"location":"Compliant%20XPU/","title":"Compliant AI Accelerators","text":"<p>This page provides an overview of the AI accelerators that have an OAAX-compliant implementations. For more details about deploying on an AI accelerator, please refer to its respective documentation page.</p>"},{"location":"Compliant%20XPU/#cpu","title":"CPU","text":"<ul> <li>Intel CPUs with AVX2 instruction set</li> <li>AMD CPUs with AVX2 instruction set</li> <li>ARM CPUs with NEON instruction set</li> </ul>"},{"location":"Compliant%20XPU/#intel","title":"Intel","text":"<ul> <li>CPUs with AVX2 instruction set</li> <li>GPU</li> <li>NPU</li> </ul>"},{"location":"Compliant%20XPU/#nvidia","title":"NVIDIA","text":"<ul> <li>GPUs with CUDA support 11 or 12, ie. GPUs with compute capability \u2265 3.5</li> </ul>"},{"location":"Compliant%20XPU/#deepx","title":"DEEPX","text":"<ul> <li>DX-M1</li> <li>DX-H1</li> </ul>"},{"location":"Compliant%20XPU/#hailo","title":"HAILO","text":"<ul> <li>Hailo-8</li> <li>Hailo-8L</li> </ul>"},{"location":"DEEPX/","title":"DEEPX","text":""},{"location":"DEEPX/#overview","title":"Overview","text":"<p>DEEPX is a leading on-device AI semiconductor company specializing in Neural Processing Units (NPUs), headquartered in South Korea. They offer two types of NPUs: DX-M1 and DX-H1, designed for efficient AI inference in edge devices, where the latter is the high-performance variant. </p> <p>For more information, please visit DEEPX's official website.</p>"},{"location":"DEEPX/#requirements","title":"Requirements","text":"<p>The DX-M1 module supports:</p>"},{"location":"DEEPX/#supported-operating-systems","title":"Supported Operating Systems","text":"<ul> <li>Ubuntu 20.04 or higher</li> <li>Debian 11 or higher</li> </ul>"},{"location":"DEEPX/#hardware-requirements","title":"Hardware requirements","text":"<ul> <li>CPU: x86_64 (Intel, AMD) or ARM64 (aarch64)</li> <li>RAM: 8GB (16GB or higher recommended)</li> <li>Disk Space: 4GB or more</li> <li>Slot: M.2 Key-M (PCIe Gen3 x4 recommended)</li> </ul>"},{"location":"DEEPX/#driver-installation","title":"Driver installation","text":"<p>To install the DX-M1 module, insert it into an available M.2 (Key M) slot on the target system. The required power (up to 3.3V / 3A) is delivered through the M.2 interface, so no external power connection is needed. After installing the module, install the following components from the DEEPX SDK, which are publicly available on GitHub:</p> <ul> <li>DX Runtime (DX-RT)</li> <li>NPU Driver</li> <li>Firmware (FW)</li> </ul> <p>Make sure to install DXRT v3.0.0 version.</p> <p>After installation, run the following command to verify proper setup:</p> <pre><code>dxrt-cli --status\n</code></pre> <p>Make sure the versions of the runtime, driver, and firmware are displayed correctly. You should see output similar to the following:</p> <pre><code>DXRT v3.0.0\n=======================================================\n * Device 0: M1, Accelerator type\n---------------------   Version   ---------------------\n * RT Driver version   : v1.7.1\n * PCIe Driver version : v1.4.1\n-------------------------------------------------------\n * FW version          : v2.1.4\n--------------------- Device Info ---------------------\n * Memory : LPDDR5 6000 Mbps, 3.92GiB\n * Board  : M.2, Rev 1.5\n * Chip Offset : 0\n * PCIe   : Gen3 X4 [08:00:00]\n\nNPU 0: voltage 750 mV, clock 1000 MHz, temperature 39'C\nNPU 1: voltage 750 mV, clock 1000 MHz, temperature 39'C\nNPU 2: voltage 750 mV, clock 1000 MHz, temperature 39'C\n=======================================================\n</code></pre>"},{"location":"DEEPX/#supported-models","title":"Supported Models","text":"<p>The DEEPX SDK includes an Open Model Zoo that provides various pre-optimized neural network models for DEEPX NPUs.</p>"},{"location":"DEEPX/#usage","title":"Usage","text":""},{"location":"DEEPX/#runtime-library","title":"Runtime Library","text":"<p>The runtime implements the standard OAAX interface, without any initialization arguments. Therefore the <code>int runtime_initialization_with_args</code> function is not supported, and the <code>int runtime_initialization()</code> function should be used instead.</p>"},{"location":"DEEPX/#conversion-toolchain","title":"Conversion Toolchain","text":"<p>The DEEPX conversion toolchain doesn't come with DX COM by default, it needs to be manually downloaded and unpacked on the machine where the conversion will be performed. You can find details about the process in the DEEPX conversion toolchain GitHub repository.</p> <p>Make sure to download DX COM 2.0.0 version which is compatible with DXRT v3.0.0</p> <p>To convert models for use with DEEPX Accelerators, models must be packaged as a ZIP archive. For instructions on preparing the archive, refer to this GitHub guide: Prepare Your Model. For guidance on JSON configuration files and supported ONNX operators, refer to the following documents: - JSON File Configuration, or refer to the model zoo for examples. - Supported ONNX Operations</p> <p>Once DX COM is downloaded and the model archive is prepared, you can run the conversion using the following command:</p> <pre><code>docker run -v ./model:/app/artifacts -v ./dx_com:/app/dx_com oaax-deepx-toolchain:latest /app/artifacts/model.zip /app/artifacts/output\n</code></pre> <p>This command assumes that the model archive is located at <code>./model/model.zip</code> and the DX COM directory is located at <code>./dx_com</code>. The converted model will be saved in the <code>./model/output</code> directory.</p>"},{"location":"DEEPX/#download-links-and-compatibility-matrix","title":"Download links and compatibility matrix","text":"<p>Please make sure that you're using DXRT v3.0.0 version on the host-running machine.</p> OAAX versions OS Version CPU architecture Runtime library Conversion toolchain 1.1.0 Ubuntu/Debian 20.04/11 x86_64 Download Download 1.1.0 Ubuntu/Debian 20.04/11 ARM64 Download \u2b06\ufe0f 1.1.0 Ubuntu/Debian 22.04/12 x86_64 Download \u2b06\ufe0f 1.1.0 Ubuntu/Debian 22.04/12 ARM64 Download \u2b06\ufe0f 1.1.0 Ubuntu/Debian 24.04/13 x86_64 Download \u2b06\ufe0f 1.1.0 Ubuntu/Debian 24.04/13 ARM64 Download \u2b06\ufe0f"},{"location":"FAQ/","title":"\u2753 OAAX Frequently Asked Questions (FAQ)","text":"<p>Welcome to the OAAX FAQ. Below are answers to common questions about the OAAX standard, its goals, usage, and community.</p>"},{"location":"FAQ/#general","title":"\ud83e\udde0 General","text":"<p>What is OAAX? OAAX (Open AI Accelerator eXchange) is an open standard for deploying AI models across heterogeneous hardware (aka. XPUs) using a unified conversion and runtime interface. It enables portability, reproducibility, and modularity for AI model execution at the edge and beyond.</p> <p>Why was OAAX created? AI hardware is fragmented \u2014 each vendor provides custom SDKs, runtimes, and conversion tools. OAAX addresses this fragmentation by standardizing the toolchain and runtime interface, making it easier to adopt new hardware or switch vendors without rewriting code.</p> <p>Who is behind OAAX? OAAX is a project under the LF AI &amp; Data Foundation. It is developed and maintained by a community of AI hardware vendors, software developers, and platform builders. The initial proposal was led by Network Optix.</p> <p>What does OAAX stand for? Open AI Accelerator eXchange.</p>"},{"location":"FAQ/#usage","title":"\ud83d\udd27 Usage","text":"<p>How do I use OAAX in my project?</p> <ol> <li>Convert your ONNX model using the OAAX Toolchain Docker image.</li> <li>Load the resulting OAAX bundle using the OAAX Runtime API.</li> <li>Send inputs and receive outputs through a standardized C API.</li> </ol> <p>See the Getting Started guide for full details.</p> <p>What model formats are supported? Currently, OAAX supports ONNX format as input for model conversion. The output is a binary archive specific to the selected AI hardware.</p> <p>Is OAAX only for inference? Yes. OAAX currently targets inference workloads only. Training workloads are out of scope.</p>"},{"location":"FAQ/#technical-details","title":"\ud83d\udca1 Technical Details","text":"<p>How does the runtime interface work? OAAX defines a minimal C API with functions such as <code>runtime_initialization</code>, <code>runtime_model_loading</code>, <code>send_input</code>, and <code>receive_output</code>. Each hardware vendor implements this API to allow their device to run OAAX-converted models.</p> <p>Is the toolchain containerized? Yes. The conversion toolchain is packaged as a Docker image to isolate dependencies and ensure reproducibility. This also abstracts away vendor-specific conversion scripts.</p> <p>Can I run multiple OAAX runtimes together? Yes. Because the runtime interface is standardized and stateless, you can instantiate multiple runtimes \u2014 each targeting a different accelerator \u2014 in a single application.</p> <p>What platforms does OAAX support?</p> <ul> <li>Host OS: Linux (Ubuntu), Windows (limited)</li> <li>Accelerators: CPU, Intel, NVIDIA, Hailo, DEEPX, and more in progress.</li> <li>Languages: C/C++ (runtime), Python bindings coming soon</li> </ul> <p>Does OAAX support dynamic batching or streaming? Support for batching is vendor-specific. The standard itself allows for single or batched inputs; runtime implementations may offer optimizations.</p>"},{"location":"FAQ/#contributions","title":"\ud83d\udce6 Contributions","text":"<p>How do I add support for a new accelerator? Vendors can contribute:</p> <ul> <li>A toolchain Docker image that consumes ONNX and produces a valid bundle that can be loaded by the OAAX runtime.</li> <li>A runtime implementation of the OAAX C API   See the Contributor Guide for instructions.</li> </ul> <p>Can I propose changes to the OAAX spec? Yes. OAAX is an open standard and encourages discussion via GitHub issues, pull requests, and mailing lists. Major changes go through the steering committee for review.</p> <p>Where is the source code? All repositories are on the OAAX GitHub organization.</p>"},{"location":"FAQ/#compatibility","title":"\ud83e\uddea Compatibility","text":"<p>Is OAAX compatible with OpenVINO, TensorRT, or other toolkits? Yes, OAAX can act as a wrapper around vendor toolkits. For example, the Intel OAAX runtime internally calls OpenVINO. The goal is to standardize the outer interface while allowing flexibility inside.</p> <p>Does OAAX guarantee performance parity with native SDKs? OAAX introduces a thin wrapper layer. While it may add negligible overhead, the performance largely matches native execution \u2014 assuming the vendor has optimized their runtime properly.</p>"},{"location":"FAQ/#ip-and-deployment","title":"\ud83d\udd10 IP and Deployment","text":"<p>Can I keep my model private when using OAAX? Yes. OAAX as a standard allows keeping models private. It's up to the vendor's implementation to handle model compilation (by the toolchain) and loading (by the runtime) securely.</p> <p>Is the runtime sandboxed? No, the runtime runs natively. However, you can deploy it inside containers, isolated services, or with limited permissions.</p>"},{"location":"FAQ/#community","title":"\ud83d\udce3 Community","text":"<p>How do I get help?</p> <ul> <li>GitHub Discussions: OAAX-standard/OAAX</li> <li>Slack: Coming soon</li> </ul> <p>How often does the standard change? OAAX aims for stable, versioned releases. Breaking changes are minimized to maintain compatibility.</p>"},{"location":"FAQ/#licensing-legal","title":"\ud83e\uddfe Licensing &amp; Legal","text":"<p>What license is OAAX released under? OAAX is licensed under the Apache 2.0 License, encouraging open contribution and commercial use.</p> <p>Have a question that isn't answered here? \ud83d\udcac Open a GitHub Issue to help us improve this FAQ.</p>"},{"location":"Getting%20Started/","title":"Hello World Example","text":"<p>To deploy an AI model on an OAAX-compliant AI Accelerator, the first step is to ensure that the model is compatible with the AI accelerator. That can be achieved by checking their documentation or attempting to convert the model using their conversion toolchain. If the conversion is successful, the next step is to ensure that the runtime-hosting machine is properly set up with the necessary software and dependencies. Refer to the documentation of the XPU to validate the setup.</p> <p>That said, to illustrate the OAAX workflow, let's consider the example of running the YOLOv8 model from Ultralytics on Intel CPU, NVIDIA GPU, and DEEPX.</p>"},{"location":"Getting%20Started/#requirements","title":"Requirements","text":"<ul> <li>An x86_64 machine where the model conversion will take place.</li> <li>A machine (or machines) with the AI accelerators installed.</li> </ul>"},{"location":"Getting%20Started/#deployment-steps","title":"Deployment steps","text":"<p>The following steps outline the process of deploying the YOLOv8 model on Intel CPU, NVIDIA GPU, and DEEPX using OAAX:</p> <ol> <li>Export the model from PyTorch to ONNX.</li> <li>Download the conversion toolchain for the target XPUs: Intel, NVIDIA, DEEPX.</li> <li>Convert the model using the conversion toolchain for each XPU.</li> <li>Set up the runtime-hosting machines by installing the necessary OAAX runtime libraries and dependencies.</li> <li>Run the model on each XPU using the appropriate OAAX runtime.</li> </ol>"},{"location":"Getting%20Started/#exporting-the-model-to-onnx","title":"Exporting the model to ONNX","text":"<ul> <li>Optionally, create a separate Python virtual environment for this process.</li> </ul> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\n</code></pre> <ul> <li>Install the Ultralytics using:</li> </ul> <pre><code>pip install ultralytics\n</code></pre> <ul> <li>Install the ONNX package</li> </ul> <pre><code>pip install onnx==1.15.0\n</code></pre> <ul> <li>Export the model to ONNX format using the following command:</li> </ul> <pre><code>yolo export model=yolov8n.pt format=onnx\n</code></pre> <p>This process will generate a file named <code>yolov8n.onnx</code> that we'll later use as a source model for the conversion toolchains.</p>"},{"location":"Getting%20Started/#download-the-conversion-toolchain","title":"Download the conversion toolchain","text":"<p>You can download the conversion toolchain for each XPU from their respective documentation page or using the links below:</p> <ul> <li>Intel toolchain</li> <li>NVIDIA toolchain</li> <li>DEEPX toolchain</li> </ul> <p>To download them on Linux, you can use the following commands:</p> <pre><code>wget https://oaax.nbg1.your-objectstorage.com/conversion-toolchain/latest/INTEL/oaax-intel-toolchain.tar\nwget https://oaax.nbg1.your-objectstorage.com/conversion-toolchain/latest/NVIDIA/oaax-nvidia-toolchain.tar\nwget https://oaax.nbg1.your-objectstorage.com/conversion-toolchain/latest/DEEPX/oaax-deepx-toolchain.tar\n</code></pre> <p>Now, you can load them into Docker using the following commands:</p> <pre><code>docker load -i oaax-intel-toolchain.tar\ndocker load -i oaax-nvidia-toolchain.tar\ndocker load -i oaax-deepx-toolchain.tar\n</code></pre>"},{"location":"Getting%20Started/#convert-the-model","title":"Convert the model","text":"<p>Both NVIDIA and Intel toolchains can take in the ONNX model as input directly. However, the DEEPX toolchain requires the input to be an archive (.zip) containing the ONNX model, a JSON configuration file, and calibration images to perform quantization.</p> <p>Make sure the ONNX model is an directory named \"model\" containing the <code>yolov8n.onnx</code> file.</p> <p>You can use the following commands to perform the conversion for each XPU:</p>"},{"location":"Getting%20Started/#intel","title":"Intel","text":"<p>The conversion can be initiated using the following command:</p> <pre><code>docker run --rm -v ./model:/tmp/run oaax-intel-toolchain:1.1.1 /tmp/run/yolov8n.onnx /tmp/run/intel-output/\n</code></pre> <p>The output ONNX will be located in the <code>./model/intel-output/yolov8n-simplified.onnx</code> directory.</p>"},{"location":"Getting%20Started/#nvidia","title":"NVIDIA","text":"<p>The conversion can be initiated using the following command:</p> <pre><code>docker run --rm -v ./model:/tmp/run oaax-nvidia-toolchain:1.1.1 /tmp/run/yolov8n.onnx /tmp/run/nvidia-output/\n</code></pre> <p>The output ONNX will be located in the <code>./model/output/yolov8n-simplified.onnx</code> directory.</p>"},{"location":"Getting%20Started/#deepx","title":"DEEPX","text":"<p>To prepare the zip archive, we'll need to use the following JSON configuration file. Save it as <code>deepx-config.json</code> in the <code>./model</code> directory.</p> deepx-config.json <pre><code>{\n  \"inputs\": {\n    \"images\": [\n      1,\n      3,\n      640,\n      640\n    ]\n  },\n  \"calibration_num\": 100,\n  \"calibration_method\": \"ema\",\n  \"train_batchsize\": 32,\n  \"num_samples\": 1024,\n  \"default_loader\": {\n    \"dataset_path\": \"/app/dataset\",\n    \"file_extensions\": [\n      \"jpeg\",\n      \"jpg\",\n      \"png\",\n      \"JPEG\"\n    ],\n    \"preprocessings\": [\n      {\n        \"resize\": {\n          \"mode\": \"pad\",\n          \"size\": 640,\n          \"pad_location\": \"edge\",\n          \"pad_value\": [\n            114,\n            114,\n            114\n          ]\n        }\n      },\n      {\n        \"div\": {\n          \"x\": 255\n        }\n      },\n      {\n        \"convertColor\": {\n          \"form\": \"BGR2RGB\"\n        }\n      },\n      {\n        \"transpose\": {\n          \"axis\": [\n            2,\n            0,\n            1\n          ]\n        }\n      },\n      {\n        \"expandDim\": {\n          \"axis\": 0\n        }\n      }\n    ]\n  }\n}\n</code></pre> <p>In addition, you'll need to prepare a dataset of images for calibration. Save the images in a directory named <code>dataset</code> inside the <code>./model</code> directory.</p> <p>Now, you can create the zip archive using the following command:</p> <pre><code>cd model\nzip -r deepx-yolov8n.zip yolov8n.onnx deepx-config.json dataset\n</code></pre> <p>The next step is to download the DX COM SDK from DEEPX developer portal, since it's not included by default in the conversion toolchain. You can download it from here. Once it is downloaded and unpacked next to the <code>./model</code> directory, you can run the conversion using the following command:</p> <pre><code>docker run --rm -v ./model:/app/artifacts -v ./dx_com:/app/dx_com oaax-deepx-toolchain:latest /app/artifacts/deepx-yolov8n.zip /app/artifacts/deepx-output/\n</code></pre> <p>The compiled model will be located at <code>./model/deepx-output/yolov8n.dxnn</code>.</p>"},{"location":"Getting%20Started/#summary","title":"Summary","text":"<p>At this point, you should have the following files inside the <code>./model</code> directory:</p> <ul> <li><code>yolov8n.onnx</code>: The original ONNX model exported from Ultralytics framework.</li> <li><code>intel-output/yolov8n-simplified.onnx</code>: The optimized ONNX model for Intel CPUs.</li> <li><code>nvidia-output/yolov8n-simplified.onnx</code>: The optimized ONNX model for NVIDIA GPUs.</li> <li><code>deepx-output/yolov8n.dxnn</code>: The compiled model for DEEPX AI accelerator.</li> </ul>"},{"location":"Getting%20Started/#set-up-the-runtime-hosting-machines","title":"Set up the runtime-hosting machines","text":"<p>This step involves installing the necessary driver and SDK libraries for each AI accelerator on the respective runtime-hosting machines. Please refer to the official documentation of each XPU for detailed instructions on how to set up the environment.</p> <p>Note: Ensure you have hardware that is supported by the respective XPU, and that you install the supported versions of the drivers and SDKs.</p>"},{"location":"Getting%20Started/#run-the-model","title":"Run the model","text":"<p>Finally, the last step is to run the model on each XPU using the appropriate runtime. Please consult each XPU's page for the appropriate runtime library to use based on your hardware and software environment:</p> <ul> <li>Intel runtime</li> <li>NVIDIA runtime</li> <li>DEEPX runtime</li> </ul> <p>After downloading the appropriate runtime, you can use the YOLOv8 inference example to run the model on each XPU. Make sure to update the paths in the <code>build-and-run.sh</code> script accordingly.</p> <p>On a successful run, you should see an output similar to the following if you're using the NVIDIA runtime:</p> <pre><code>[2025-09-15 20:20:01.932] [OAAX] [info] Initializing OAAX inference engine with the following parameters:\n[2025-09-15 20:20:01.932] [OAAX] [info] Library Path: ./artifacts/libRuntimeLibrary.so\n[2025-09-15 20:20:01.932] [OAAX] [info] Model Path: ./model/nvidia-output/yolov8n-simplified.onnx\n[2025-09-15 20:20:01.932] [OAAX] [info] Input Path: ../artifacts/image.jpg\n[2025-09-15 20:20:01.932] [OAAX] [info] Configuration Path: ../model-configs/nvidia-model.json\n[2025-09-15 20:20:01.932] [OAAX] [info] Log File: app.log\n[2025-09-15 20:20:01.932] [OAAX] [info] Log Level: 2\n[2025-09-15 20:20:01.943] [OAAX] [info] Runtime Name: OAAX NVIDIA Runtime\n[2025-09-15 20:20:01.943] [OAAX] [info] Runtime Version: 1.1.1\n[2025-09-15 20:20:01.945] [OAAX NVIDIA Runtime] [info] Initializing the runtime\n[2025-09-15 20:20:02.032] [OAAX] [info] Runtime initialized successfully.\n[2025-09-15 20:20:02.032] [OAAX NVIDIA Runtime] [info] Inference thread started\n[2025-09-15 20:20:02.032] [OAAX NVIDIA Runtime] [info] Runtime arguments:\n[2025-09-15 20:20:02.032] [OAAX NVIDIA Runtime] [info]   log_level: 2\n[2025-09-15 20:20:02.032] [OAAX NVIDIA Runtime] [info]   log_file: runtime.log\n[2025-09-15 20:20:02.032] [OAAX NVIDIA Runtime] [info]   num_threads: 4\n[2025-09-15 20:20:02.032] [OAAX NVIDIA Runtime] [info] Loading model from: ./model/nvidia-output/yolov8n-simplified.onnx\n[2025-09-15 20:20:02.083] [OAAX] [info] Model loaded successfully: ./model/nvidia-output/yolov8n-simplified.onnx\n[2025-09-15 20:20:02.083] [OAAX] [info] Configuration: {\n    \"model\": {\n        \"input_dtype\": \"float32\",\n        \"input_height\": 640,\n        \"input_name\": \"images\",\n        \"input_width\": 640,\n        \"mean\": [\n            0,\n            0,\n            0\n        ],\n        \"nchw\": 1,\n        \"std\": [\n            255,\n            255,\n            255\n        ]\n    }\n}\n[2025-09-15 20:20:02.083] [OAAX] [info] Preprocessing image: ../artifacts/image.jpg\n[2025-09-15 20:20:02.087] [OAAX] [info] Mean: 0, 0, 0\n[2025-09-15 20:20:02.087] [OAAX] [info] Stddev: 255, 255, 255\n[2025-09-15 20:20:02.090] [OAAX] [info] Image min value: 0, max value: 1\n[2025-09-15 20:20:02.090] [OAAX] [info] Creating tensors for input image: images\nNumber of tensors: 1\nTensor id=0:\n  Name: 'images'\n  Data type: 1\n  Rank: 4\n  Shape: 1 3 640 640 \n  Data Pointer: 0x7f2b5574e010\n\n[2025-09-15 20:20:02.092] [OAAX] [info] Starting input sending and output receiving threads...\n[2025-09-15 20:20:02.092] [OAAX] [info] Sending input tensors to the runtime...\n[2025-09-15 20:20:02.092] [OAAX] [info] Waiting for threads to finish...\n[2025-09-15 20:20:02.093] [OAAX] [info] Sent input tensors: 1\n[2025-09-15 20:20:02.095] [OAAX] [info] Sent input tensors: 2\n[2025-09-15 20:20:02.096] [OAAX] [info] Sent input tensors: 3\n[2025-09-15 20:20:02.098] [OAAX] [info] Sent input tensors: 4\n[2025-09-15 20:20:02.100] [OAAX] [info] Sent input tensors: 5\n[2025-09-15 20:20:02.101] [OAAX] [info] Sent input tensors: 6\n[2025-09-15 20:20:02.103] [OAAX] [info] Sent input tensors: 7\n[2025-09-15 20:20:02.105] [OAAX] [info] Sent input tensors: 8\n[2025-09-15 20:20:02.107] [OAAX] [info] Sent input tensors: 9\n[2025-09-15 20:20:02.108] [OAAX] [info] Sent input tensors: 10\n[2025-09-15 20:20:02.108] [OAAX] [info] All input tensors sent successfully.\n[2025-09-15 20:20:03.093] [OAAX] [info] Output tensors received: 1\n[2025-09-15 20:20:03.093] [OAAX] [info] Output tensors received: 2\n[2025-09-15 20:20:03.093] [OAAX] [info] Output tensors received: 3\n[2025-09-15 20:20:03.093] [OAAX] [info] Output tensors received: 4\n[2025-09-15 20:20:03.093] [OAAX] [info] Output tensors received: 5\n[2025-09-15 20:20:03.093] [OAAX] [info] Output tensors received: 6\n[2025-09-15 20:20:03.093] [OAAX] [info] Output tensors received: 7\n[2025-09-15 20:20:03.093] [OAAX] [info] Output tensors received: 8\n[2025-09-15 20:20:03.093] [OAAX] [info] Output tensors received: 9\n[2025-09-15 20:20:03.093] [OAAX] [info] Terminating OAAX inference engine.\n[2025-09-15 20:20:03.093] [OAAX] [info] Output tensors received: 10\n[2025-09-15 20:20:03.093] [OAAX] [info] Output tensors received successfully.\n[2025-09-15 20:20:03.093] [OAAX] [info] Threads finished successfully.\n[2025-09-15 20:20:03.093] [OAAX NVIDIA Runtime] [info] Destroying runtime...\n</code></pre>"},{"location":"Intel/","title":"Intel","text":""},{"location":"Intel/#overview","title":"Overview","text":"<p>OAAX provides a runtime and conversion toolchain for running ONNX models on Intel CPUs, GPUs and NPUs on both Ubuntu and Windows.</p> <p>It uses the ONNX runtime and OpenVINO\u2122 toolkit under the hood to offload the computation to Intel hardware. For more information about OpenVINO please visit the OpenVINO website.</p>"},{"location":"Intel/#requirements","title":"Requirements","text":""},{"location":"Intel/#hardware","title":"Hardware","text":"<p>Please consult the OpenVINO documentation for the most up-to-date hardware requirements.</p>"},{"location":"Intel/#operating-system","title":"Operating System","text":"<ul> <li>Debian 11 or higher</li> <li>Ubuntu 20.04 or higher</li> <li>Windows 10, 11</li> </ul>"},{"location":"Intel/#installation","title":"Installation","text":"<p>To be able to use the Intel GPU and NPU, please make sure to install the latest drivers from Intel:</p> <ul> <li>For GPU, please follow the instructions here.</li> <li>For NPU, please follow the instructions here.</li> </ul>"},{"location":"Intel/#usage","title":"Usage","text":""},{"location":"Intel/#runtime-library","title":"Runtime Library","text":"<p>The runtime library implements the OAAX's runtime interface for initializing, loading, running inference and destroying the runtime. The initialization in particular can be done without providing a configuration by calling <code>int runtime_initialization();</code> directly or by providing these parameters to <code>int runtime_initialization_with_args(int length, char **keys, void **values);</code>:</p> <ul> <li><code>log_level</code> (char *, default is \"2\"): The minimal log level for the runtime. This can be set to <code>0</code> for trace, <code>1</code> for debug, <code>2</code> for info, <code>3</code> for warnings, <code>4</code> for errors, <code>5</code> for critical and <code>6</code> to disable logging.</li> <li><code>log_file</code> (char *, default is \"runtime.log\"): The file to which the runtime logs will be written. If not specified, logs will be written to stdout.</li> <li><code>num_threads</code> (char *, default is \"8\"): The maximum number of threads that can be used by the runtime. The higher the number, the more CPU resources will be used, but the better the throughput.</li> <li><code>device_type</code> (char *, default is \"CPU\"): The type of device to use for inference. This can be set to <code>CPU</code>, <code>GPU</code>, or <code>NPU</code>. The default is <code>CPU</code>.</li> <li><code>precision</code> (char *, default is \"FP32\"): The precision to use for inference. This can be set to <code>FP32</code>, <code>FP16</code>, or <code>ACCURACY</code>. The default is <code>FP32</code>.</li> </ul> <p>You can check out the examples repository for more details on how to use the runtime library: OAAX Examples.</p>"},{"location":"Intel/#conversion-toolchain","title":"Conversion Toolchain","text":"<p>The conversion toolchain is used to validate, optimize and simplify the ONNX models. At the end of the process it produces a simplified ONNX model.</p> <p>It can be used as follows:</p> <pre><code>docker run -v ./model:/model  oaax-intel-toolchain:1.1.1 /model/model.onnx /model/output\n</code></pre> <p>The above command assumes that the model is located at <code>./model/model.onnx</code>. After a succesful conversion, the generated model will be saved in the <code>./model/output</code> directory.</p>"},{"location":"Intel/#download-links-and-compatibility-matrix","title":"Download links and compatibility matrix","text":"OAAX versions OS Version CPU architecture Runtime library Conversion toolchain 1.1.1 Ubuntu 22.04, 24.04 x86_64 Download Download 1.1.1 Windows 10, 11 x86_64 Download Download"},{"location":"NVIDIA/","title":"NVIDIA","text":""},{"location":"NVIDIA/#overview","title":"Overview","text":"<p>For NVIDIA GPUs, OAAX provides an implementation of conversion toolchain and runtime library. The latter is based on the ONNX Runtime with CUDA Execution Provider.</p>"},{"location":"NVIDIA/#requirements","title":"Requirements","text":"<p>Most NVIDIA GPUs are supported, including:</p> <ul> <li>Data Center / Desktop GPUs with compute capability 3.5 or higher (e.g., T4, A10, A100, RTX 3000 series, RTX 4000 series) running on Ubuntu 20.04 or higher or Windows 10/11.</li> <li>NVIDIA Jetson series (e.g. TX2, Xavier NX, AGX Orin) running JetPack 5.0 or higher.</li> </ul>"},{"location":"NVIDIA/#installation","title":"Installation","text":"<p>To use the NVIDIA GPU acceleration, please ensure that you have the latest NVIDIA drivers and CUDA toolkit installed: - For Data Center / Desktop GPUs, please follow the instructions here. - For NVIDIA Jetson series, please follow the instructions here.</p>"},{"location":"NVIDIA/#usage","title":"Usage","text":""},{"location":"NVIDIA/#runtime-library","title":"Runtime Library","text":"<p>The runtime library implements the OAAX's runtime interface for initializing, loading, running inference and destroying the runtime. The initialization in particular can be done without providing a configuration by calling <code>int runtime_initialization();</code> directly or by providing these parameters to <code>int runtime_initialization_with_args(int length, char **keys, void **values);</code>:</p> <ul> <li><code>log_level</code> (char *, default is \"2\"): The minimal log level for the runtime. This can be set to <code>0</code> for trace, <code>1</code> for debug, <code>2</code> for info, <code>3</code> for warnings, <code>4</code> for errors, <code>5</code> for critical and <code>6</code> to disable logging.</li> <li><code>log_file</code> (char *, default is \"runtime.log\"): The file to which the runtime logs will be written. If not specified, logs will be written to stdout.</li> <li><code>num_threads</code> (char *, default is \"4\"): The maximum number of threads that can be used by the runtime. The higher the number, the more CPU resources will be used, but the better the throughput.</li> </ul> <p>You can check out the examples repository for more details on how to use the runtime library: OAAX Examples.</p>"},{"location":"NVIDIA/#conversion-toolchain","title":"Conversion Toolchain","text":"<p>The conversion toolchain is used to validate, optimize and simplify the ONNX models. At the end of the process it produces a simplified ONNX model.</p> <p>It can be used as follows:</p> <pre><code>docker run -v ./model:/model  oaax-nvidia-toolchain:1.1.1 /model/model.onnx /model/output\n</code></pre> <p>The above command assumes that the model is located at <code>./model/model.onnx</code>. After a succesful conversion, the generated model will be saved in the <code>./model/output</code> directory.</p>"},{"location":"NVIDIA/#download-links-and-compatibility-matrix","title":"Download links and compatibility matrix","text":"OAAX versions OS Version CPU architecture Runtime library Conversion toolchain 1.1.1 Ubuntu 20.04 or higher x86_64 CUDA 11, CUDA 12 Download 1.1.1 NVIDIA JetPack - ARM64 JetPack 5, JetPack 6 \u2b06\ufe0f 1.1.1 Windows 10 or higher x86_64 CUDA 11, CUDA 12 \u2b06\ufe0f"},{"location":"Specification/","title":"\ud83d\udcdc OAAX Specification (version 1.0)","text":"<p>This document outlines the official specification for OAAX (Open AI Accelerator eXchange). It describes the structure, components, and interfaces that constitute a valid OAAX implementation, including the toolchain and runtime.</p>"},{"location":"Specification/#core-components","title":"Core Components","text":"<p>OAAX consists of two core components:</p> Component Description Specification OAAX Toolchain Docker-based conversion pipeline that takes an ONNX model and outputs a binary targeting a specific XPU Specs OAAX Runtime A shared library (.so or .dll) that implements the OAAX runtime API and executes the binary of a specific XPU Specs"},{"location":"Specification/#oaax-toolchain-specification","title":"OAAX Toolchain Specification","text":"<p>This section defines the requirements and interface for OAAX-compliant toolchain implementations. All OAAX toolchains must conform to this specification to ensure compatibility and interoperability within the OAAX ecosystem.</p>"},{"location":"Specification/#1-overview","title":"1. Overview","text":"<p>The OAAX toolchain is a Docker-based conversion pipeline responsible for transforming hardware-agnostic AI models (in ONNX format) into XPU-specific optimized formats. The toolchain encapsulates all necessary dependencies, tools, and conversion logic within a containerized environment, ensuring:</p> <ul> <li>Portability: Cross-platform execution capability</li> <li>Isolation: Clean, reproducible conversion environments</li> <li>Flexibility: Freedom for XPU manufacturers to use proprietary conversion tools</li> <li>Standardization: Uniform interface regardless of underlying implementation</li> </ul> <p>The toolchain formally implements the conversion function: <code>f : M \u2192 M_xpu</code>, where <code>M</code> represents the generic ONNX model, and <code>M_xpu</code> represents the XPU-specific optimized model.</p>"},{"location":"Specification/#2-docker-container-requirements","title":"2. Docker Container Requirements","text":""},{"location":"Specification/#21-container-structure","title":"2.1 Container Structure","text":"<p>Every OAAX toolchain must be packaged as a Docker image with the following characteristics:</p> <ul> <li>Working Directory: <code>/app</code> (recommended)</li> <li>Entry Point: Must accept exactly two command-line arguments</li> <li>Network Access: Container should not require network connectivity</li> <li>User Permissions: Must not require root privileges for normal operation</li> </ul>"},{"location":"Specification/#22-entry-point-specification","title":"2.2 Entry Point Specification","text":"<p>The Docker container's entry point must implement the following signature:</p> <pre><code>&lt;toolchain-entrypoint&gt; &lt;input-path&gt; &lt;output-directory&gt;\n</code></pre> <p>Parameters: * <code>&lt;input-path&gt;</code>: Absolute path to the input model file or archive within the container * <code>&lt;output-directory&gt;</code>: Absolute path to the directory where conversion outputs will be written. If the directory does not exist, it must be created by the toolchain.</p> <p>Example Usage:</p> <pre><code>docker run -v \"$PWD:/app/run\" oaax-toolchain-vendor \"/app/run/model.onnx\" \"/app/run/build\"\n</code></pre>"},{"location":"Specification/#3-input-requirements","title":"3. Input Requirements","text":""},{"location":"Specification/#31-supported-input-formats","title":"3.1 Supported Input Formats","text":"<p>OAAX toolchains must support at least one of the following input formats:</p> <p>Primary Format: * ONNX Model File (<code>.onnx</code>): Standard ONNX model representation</p> <p>Extended Format (Optional): * Archive File (<code>.zip</code>, <code>.tar.gz</code>): Containing ONNX model plus supplementary files such as:   * Calibration datasets for quantization   * Configuration files   * Validation datasets</p>"},{"location":"Specification/#32-input-validation","title":"3.2 Input Validation","text":"<p>Toolchains must perform input validation and provide meaningful error messages for:</p> <ul> <li>File Format Validation: Verify ONNX model integrity</li> <li>Operator Support: Check if all required operators are supported</li> <li>Model Architecture: Validate compatibility with target XPU capabilities</li> <li>Input Constraints: Verify model meets XPU-specific limitations (memory, precision, etc.)</li> </ul>"},{"location":"Specification/#4-conversion-process","title":"4. Conversion Process","text":"<p>Toolchains should provide progress information through:</p> <ul> <li>Console Output: Real-time conversion status messages</li> <li>Error Reporting: Human-readable error messages with actionable guidance</li> </ul>"},{"location":"Specification/#5-output-requirements","title":"5. Output Requirements","text":""},{"location":"Specification/#51-mandatory-outputs","title":"5.1 Mandatory Outputs","text":"<p>Every successful toolchain execution must produce:</p> <p>1. Model File: XPU-specific model representation * Filename: Flexible naming (e.g., <code>model.oaax</code>, <code>model.bin</code>, <code>optimized_model.zip</code>) * Validation: Must be loadable by corresponding OAAX runtime * I/O Compatibility: Must retain input/output tensor names and shapes of the original ONNX model</p> <p>2. Conversion Log: JSON-formatted processing log * Filename: <code>conversion-log.json</code> or <code>logs.json</code> * Content: Structured information about the conversion process</p>"},{"location":"Specification/#52-output-validation","title":"5.2 Output Validation","text":"<p>Toolchains must ensure:</p> <ul> <li>File Integrity: All output files are complete and valid</li> <li>Metadata Accuracy: Log information correctly reflects actual conversion results</li> </ul>"},{"location":"Specification/#6-error-handling","title":"6. Error Handling","text":""},{"location":"Specification/#61-exit-codes","title":"6.1 Exit Codes","text":"<p>Toolchains must use standard exit codes:</p> <ul> <li>0: Successful conversion,</li> <li>Non-zero: otherwise.</li> </ul>"},{"location":"Specification/#62-error-reporting","title":"6.2 Error Reporting","text":"<p>For all failure cases, toolchains must:</p> <ul> <li>Generate Logs: Produce conversion-log.json with error details</li> <li>Provide Context: Include actionable error messages</li> <li>Maintain Consistency: Use standardized error codes and categories</li> </ul>"},{"location":"Specification/#7-container-distribution","title":"7. Container Distribution","text":""},{"location":"Specification/#71-image-naming-convention","title":"7.1 Image Naming Convention","text":"<p>OAAX toolchain containers should follow naming conventions:</p> <pre><code>oaax-&lt;vendor&gt;-toolchain[-&lt;version&gt;]:tag\n</code></pre> <p>Examples: * <code>oaax-nvidia-toolchain-v2.1:latest</code> * <code>oaax-intel-openvino-toolchain:1.0.0</code></p>"},{"location":"Specification/#8-compliance-validation","title":"8. Compliance Validation","text":""},{"location":"Specification/#81-toolchain-requirements-checklist","title":"8.1 Toolchain Requirements Checklist","text":"<p>An OAAX-compliant toolchain must:</p> <ul> <li>[ ] Accept exactly two command-line arguments (input path, output directory)</li> <li>[ ] Process valid ONNX models successfully</li> <li>[ ] Generate both model file and conversion log on success</li> <li>[ ] Use appropriate exit codes for different failure types</li> <li>[ ] Provide meaningful error messages in conversion logs</li> <li>[ ] Handle file permissions correctly across platforms</li> <li>[ ] Not require network connectivity</li> <li>[ ] Preferably, execute without root privileges</li> </ul>"},{"location":"Specification/#82-testing-and-validation","title":"8.2 Testing and Validation","text":"<p>Toolchain developers should validate their implementation using:</p> <ul> <li>Standard Test Models: Common ONNX architectures (ResNet, YOLOv5, etc.)</li> <li>Edge Cases: Invalid inputs, unsupported operators, resource constraints</li> <li>Integration Testing: Compatibility with corresponding OAAX runtime</li> <li>Cross-Platform Testing: Verification on different host operating systems</li> </ul>"},{"location":"Specification/#9-best-practices","title":"9. Best Practices","text":""},{"location":"Specification/#91-performance-optimization","title":"9.1 Performance Optimization","text":"<ul> <li>Parallel Processing: Utilize multi-core capabilities where possible</li> <li>Memory Management: Efficient memory usage for large models</li> <li>Caching: Cache intermediate results for iterative conversions</li> <li>Progress Reporting: Provide conversion progress feedback for long operations</li> </ul>"},{"location":"Specification/#92-user-experience","title":"9.2 User Experience","text":"<ul> <li>Clear Documentation: Provide comprehensive usage documentation</li> <li>Verbose Logging: Include debug-level logging options</li> <li>Model Compatibility: Clearly document supported model architectures</li> <li>Resource Requirements: Specify minimum system requirements</li> </ul>"},{"location":"Specification/#93-security-considerations","title":"9.3 Security Considerations","text":"<ul> <li>Input Sanitization: Validate all inputs to prevent security vulnerabilities</li> <li>Resource Limits: Implement reasonable memory and CPU usage limits</li> <li>File System Access: Restrict file system access to mounted volumes only</li> </ul>"},{"location":"Specification/#oaax-runtime-specification","title":"OAAX Runtime Specification","text":"<p>This section defines the Application Program Interface (API) and the structural expectations for OAAX-compliant runtime libraries. All OAAX runtimes must conform to this interface to ensure interoperability with tooling and host-side components.</p>"},{"location":"Specification/#1-overview_1","title":"1. Overview","text":"<p>The OAAX runtime interface is defined as a pure C API, facilitating dynamic linking and cross-language compatibility. Runtimes are expected to expose a predefined set of functions for:</p> <ul> <li>Runtime initialization and destruction</li> <li>Model loading</li> <li>Inference input/output</li> <li>Error and version reporting</li> </ul> <p>The runtime is loaded dynamically by the host using mechanisms such as <code>dlopen</code>/<code>dlsym</code> (POSIX) or <code>LoadLibrary</code>/<code>GetProcAddress</code> (Windows).</p>"},{"location":"Specification/#2-header-definition","title":"2. Header Definition","text":"<p>The interface is defined in the following header file: interface.h</p> <p>This file contains all required definitions. No additional symbols may be exposed by the runtime unless explicitly allowed by future revisions.</p>"},{"location":"Specification/#3-data-types","title":"3. Data Types","text":""},{"location":"Specification/#31-tensor_data_type","title":"3.1 <code>tensor_data_type</code>","text":"<p>The following enumeration defines the supported tensor element types:</p> <pre><code>typedef enum tensor_data_type {\n    DATA_TYPE_FLOAT   = 1,\n    DATA_TYPE_UINT8   = 2,\n    DATA_TYPE_INT8    = 3,\n    DATA_TYPE_UINT16  = 4,\n    DATA_TYPE_INT16   = 5,\n    DATA_TYPE_INT32   = 6,\n    DATA_TYPE_INT64   = 7,\n    DATA_TYPE_STRING  = 8,\n    DATA_TYPE_BOOL    = 9,\n    DATA_TYPE_DOUBLE  = 11,\n    DATA_TYPE_UINT32  = 12,\n    DATA_TYPE_UINT64  = 13\n} tensor_data_type;\n</code></pre>"},{"location":"Specification/#32-tensors_struct","title":"3.2 <code>tensors_struct</code>","text":"<p>A structure representing a collection of tensors, either for input or output.</p> <pre><code>typedef struct tensors_struct {\n    size_t num_tensors;           // Number of tensors\n    char** names;                 // Tensor names\n    tensor_data_type* data_types;// Data types\n    size_t* ranks;                // Number of dimensions per tensor\n    size_t** shapes;             // Shapes (dimensionality)\n    void** data;                 // Pointers to raw tensor data\n} tensors_struct;\n</code></pre> <ul> <li>Each field must be properly allocated and consistent in length across all arrays.</li> <li>Tensor shape arrays (<code>shapes[i]</code>) must contain exactly <code>ranks[i]</code> elements.</li> <li>Tensor names are UTF-8 encoded C-strings and must be unique within the list.</li> </ul>"},{"location":"Specification/#4-runtime-interface-functions","title":"4. Runtime Interface Functions","text":"<p>Each runtime must implement the following functions:</p>"},{"location":"Specification/#41-int-runtime_initializationvoid","title":"4.1 <code>int runtime_initialization(void);</code>","text":"<p>Initializes the runtime. Must be called exactly once prior to any other function, unless <code>runtime_initialization_with_args</code> is used instead.</p> <p>Returns:</p> <ul> <li><code>0</code> on success</li> <li>Non-zero value on failure</li> </ul>"},{"location":"Specification/#42-int-runtime_initialization_with_argsint-length-const-char-keys-const-void-values","title":"4.2 <code>int runtime_initialization_with_args(int length, const char **keys, const void **values);</code>","text":"<p>Initializes the runtime with key-value arguments.</p> <p>Semantics:</p> <ul> <li>If this function is used, <code>runtime_initialization()</code> must not be called.</li> <li>Unknown keys must be silently ignored.</li> <li>All arguments must remain valid for the duration of this call.</li> </ul> <p>Returns:</p> <ul> <li><code>0</code> on success</li> <li>Non-zero value on failure</li> </ul>"},{"location":"Specification/#43-int-runtime_model_loadingconst-char-file_path","title":"4.3 <code>int runtime_model_loading(const char *file_path);</code>","text":"<p>Loads a compiled OAAX model from the specified file path.</p> <p>Constraints:</p> <ul> <li>The path must point to a valid model file.</li> <li>Only one model may be loaded per runtime instance.</li> </ul> <p>Returns:</p> <ul> <li><code>0</code> on success</li> <li>Non-zero value on failure</li> </ul>"},{"location":"Specification/#44-int-send_inputtensors_struct-input_tensors","title":"4.4 <code>int send_input(tensors_struct *input_tensors);</code>","text":"<p>Transfers input tensors to the runtime for inference. The runtime must support receiving multiple input tensors sequentially without requiring the output of the previous inference to be retrieved first. This design enables efficient asynchronous processing by allowing the host to queue multiple inputs for inference, thereby maximizing throughput and minimizing idle time.</p> <p>Semantics:</p> <ul> <li>The runtime takes ownership of the <code>input_tensors</code> structure and its internal memory.</li> <li>The runtime is responsible for freeing all memory related to the inputs.</li> <li>If this function fails, the caller is responsible for memory cleanup.</li> <li>The caller must ensure that the <code>input_tensors</code> structure is valid, pre-processed and properly populated before calling this function.</li> </ul> <p>Returns:</p> <ul> <li><code>0</code> on success</li> <li>Non-zero value on failure</li> </ul>"},{"location":"Specification/#45-int-receive_outputtensors_struct-output_tensors","title":"4.5 <code>int receive_output(tensors_struct **output_tensors);</code>","text":"<p>Used to retrieve output tensors from the runtime after inference if there are any.</p> <p>Semantics:</p> <ul> <li>The runtime allocates and returns a <code>tensors_struct</code> via the provided pointer.</li> <li>The caller is responsible for freeing all memory associated with the output.</li> </ul> <p>Returns:</p> <ul> <li><code>0</code> if output was successfully retrieved</li> <li>Non-zero if no output is available or an error occurred</li> </ul>"},{"location":"Specification/#46-int-runtime_destructionvoid","title":"4.6 <code>int runtime_destruction(void);</code>","text":"<p>Finalizes and releases all internal runtime resources.</p> <p>Returns:</p> <ul> <li><code>0</code> on success</li> <li>Non-zero value on failure</li> </ul>"},{"location":"Specification/#47-const-char-runtime_error_messagevoid","title":"4.7 <code>const char *runtime_error_message(void);</code>","text":"<p>Returns a pointer to a human-readable error string describing the last error.</p> <p>Ownership:</p> <ul> <li>The returned string must be owned and freed (if necessary) by the runtime.</li> </ul>"},{"location":"Specification/#48-const-char-runtime_versionvoid","title":"4.8 <code>const char *runtime_version(void);</code>","text":"<p>Returns a static or heap-allocated string indicating the version of the runtime.</p>"},{"location":"Specification/#49-const-char-runtime_namevoid","title":"4.9 <code>const char *runtime_name(void);</code>","text":"<p>Returns a static or heap-allocated string indicating the name of the runtime.</p>"},{"location":"Specification/#5-memory-management-rules","title":"5. Memory Management Rules","text":"<ul> <li> <p>The host is responsible for freeing:</p> </li> <li> <p>Any <code>tensors_struct*</code> returned by <code>receive_output</code>.</p> </li> <li> <p>The runtime is responsible for freeing:</p> </li> <li> <p>All memory associated with input tensors passed to <code>send_input</code>, except when the function fails.</p> </li> <li>All memory internally allocated by the runtime during initialization, model loading, and inference.</li> </ul>"},{"location":"Specification/#6-host-responsibilities","title":"6. Host Responsibilities","text":"<ul> <li>Load the runtime shared library.</li> <li>Call one initialization function.</li> <li>Load the model before inference.</li> <li>Prepare and pass valid <code>tensors_struct</code> input via <code>send_input</code>.</li> <li>Retrieve and free outputs from <code>receive_output</code>.</li> <li>Finalize the runtime with <code>runtime_destruction</code>.</li> </ul>"},{"location":"Specification/#7-runtime-responsibilities","title":"7. Runtime Responsibilities","text":"<ul> <li>Properly allocate and free internal resources.</li> <li>Validate and store input tensors.</li> <li>Produce valid outputs and expose them via <code>receive_output</code>.</li> <li>Clean up upon destruction.</li> <li>Provide informative error and version strings.</li> <li>Maintain API compatibility as defined in this document.</li> </ul>"},{"location":"Specification/#change-process","title":"Change Process","text":"<p>Proposals to extend or modify the OAAX specification must be submitted via:</p> <ul> <li>GitHub pull requests or issues to OAAX-standard/OAAX</li> </ul>"}]}